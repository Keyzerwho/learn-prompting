{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Advanced Applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from config import OPENAI_API_KEY\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Define function for printing long strings as markdown\n",
    "md_print = lambda text: display(Markdown(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this section we go very powerful, more advanced applications of prompt engineering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs Using Tools\n",
    "\n",
    "**MRKL Systems (Modular Reasoning, Knowledge and Language, pronounced \"miracle\")** are a neuro-symbolic architecture that combine LLMs and enternal tools to solve complex problems. This can include calculators, APIs, custom functions, anything can be a tool!!!\n",
    "\n",
    "The MRKL systems uses an LLM as a router to determine when an external tool is needed. Here are a few examples of how a MRKL system works in action:\n",
    "\n",
    "    # Example 1: Calculator\n",
    "\n",
    "    What is 100*100?\n",
    "    CALCULATOR[100*100]\n",
    "\n",
    "\n",
    "    # Example 2: Stock Price\n",
    "\n",
    "    What is the price of Apple stock right now?\n",
    "    The current price is DATABASE[SELECT price FROM stock WHERE company = \"Apple\" AND time = \"now\"].\n",
    "    \n",
    "\n",
    "    # Example 3: Weather\n",
    "\n",
    "    What is the weather like in New York?\n",
    "    The weather is WEATHER_API[New York].\n",
    "\n",
    "\n",
    "Let's use Langchain to replicate the calculator use-case, here we use a ReAct agent to help us accomplish the tool integration MRKL demonstrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to divide two times thirty-four by three.\n",
      "Action: Calculator\n",
      "Action Input: 2 * 34 / 3\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "2 * 34 / 3\u001b[32;1m\u001b[1;3m\n",
      "```python\n",
      "print(2 * 34 / 3)\n",
      "```\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m22.666666666666668\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 22.666666666666668\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 22.666666666666668\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'22.666666666666668'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMMathChain, OpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize MathChain\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Run Query to agent\n",
    "mrkl.run(\"What is two times thirty-four divided by three?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs that Reason and Act\n",
    "\n",
    "**ReAct (reason, act)** is a paradigm for enabling language models to solve complex tasks uitilizing natural language reasoning. They reason about the actions they can perform. The workflow for the Language model in the ReAct paradigm is as follows:\n",
    "\n",
    "1. Thought (Reasoning about the question)\n",
    "2. Act (Perform an action)\n",
    "3. Observation (Observe the output of the action)\n",
    "\n",
    "These steps are looped until a \"conclusion\" is reached and the model is done with the task.\n",
    "\n",
    "This is similar to the classic RL loops of state (Action, Reward, State) which is mentioned in the ReAct paper. ReAct performs well against CoT and other methods, and is the default framework for Agent reasoning in popular LLM frameworks like Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search GPT-2 and find when it was released.\n",
      "Action: Search[GPT-2]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCould not find [GPT-2]. Similar: ['GPT-2', 'GPT-3', 'GPT-4', 'Generative pre-trained transformer', 'OpenAI', 'ChatGPT', 'GPT-J', 'AI Dungeon', 'Large language model', 'GPT2']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m GPT-2 is a type of language model, so I can search GPT-2 language model and find when it was released.\n",
      "Action: Search[GPT-2 language model]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGenerative Pre-trained Transformer 2 (GPT-2)  is an open-source artificial intelligence large language model created by OpenAI in February 2019. GPT-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, can become repetitive or nonsensical when generating long passages. It is a general-purpose learner; it was not specifically trained to do any of these tasks, and its ability to perform them is an extension of its general ability to accurately synthesize the next item in an arbitrary sequence. GPT-2 was created as a \"direct scale-up\" of OpenAI's 2018 GPT model (\"GPT-1\"), with a ten-fold increase in both its parameter count and the size of its training dataset.GPT-2 has a generative pre-trained transformer architecture which implements a deep neural network, specifically a transformer model, which uses attention in place of previous recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. This model allows for greatly increased parallelization, and outperforms previous benchmarks for RNN/CNN/LSTM-based models.OpenAI released the complete version of the GPT-2 language model (with 1.5 billion parameters) in November 2019.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m GPT-2 was released in February 2019 and the complete version was released in November 2019.\n",
      "Action: Finish[February 2019, November 2019]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'February 2019, November 2019'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "\n",
    "# Create tools\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description=\"useful for when you need to ask with lookup\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize Agent\n",
    "react = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True)\n",
    "\n",
    "# Ask a question to our ReAct Agent\n",
    "question = \"When was GPT-2 released?\"\n",
    "react.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code as Reasoning\n",
    "\n",
    "**Program-aided Language Models (PAL)** are another example of a MRKL system. The main difference is that they excecute code as opposed to external tools to solve problems. The workflow is similar to that of CoT, but PAL uses code as the intermediate reasoning, while CoT uses Natural Language.\n",
    "\n",
    "PAL actually uses some natural language reasoning in the form of comments within its code. Let's use Langchain to create a PAL Agent that will answer our math question using a few-shot prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"If a bakery has 20 loaves of bread and sells 3 loaves every hour, how many loaves will be left after the bakery is open for 5 hours?\"\"\"\n",
      "loaves_initial = 20\n",
      "loaves_sold_per_hour = 3\n",
      "hours_open = 5\n",
      "loaves_sold = loaves_sold_per_hour * hours_open\n",
      "loaves_left = loaves_initial - loaves_sold\n",
      "result = loaves_left\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Set prompt\n",
    "MATH_PROMPT = '''\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
    "\n",
    "# solution in Python:\n",
    "\"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\"\n",
    "computers_initial = 9\n",
    "computers_per_day = 5\n",
    "num_days = 4  # 4 days between monday and thursday\n",
    "computers_added = computers_per_day * num_days\n",
    "computers_total = computers_initial + computers_added\n",
    "result = computers_total\n",
    "return result\n",
    "\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "\n",
    "# solution in Python:\n",
    "\"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\n",
    "toys_initial = 5\n",
    "mom_toys = 2\n",
    "dad_toys = 2\n",
    "total_received = mom_toys + dad_toys\n",
    "total_toys = toys_initial + total_received\n",
    "result = total_toys\n",
    "\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "\n",
    "# solution in Python:\n",
    "\"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\"\"\"\n",
    "jason_lollipops_initial = 20\n",
    "jason_lollipops_after = 12\n",
    "denny_lollipops = jason_lollipops_initial - jason_lollipops_after\n",
    "result = denny_lollipops\n",
    "\n",
    "Q: If a bakery has 20 loaves of bread and sells 3 loaves every hour, how many loaves will be left after the bakery is open for 5 hours?\n",
    "\n",
    "# solution in Python:\n",
    "'''\n",
    "\n",
    "# Execute LLM to generate python solution\n",
    "llm_out = llm(MATH_PROMPT.format(question=question))\n",
    "print(llm_out)\n",
    "\n",
    "# Execute python code\n",
    "exec(llm_out)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
