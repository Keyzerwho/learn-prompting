{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ² Miscellaneous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/miesner.jacob/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from transformers import pipeline\n",
    "from config import OPENAI_API_KEY\n",
    "import openai\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Define function for printing long strings as markdown\n",
    "md_print = lambda text: display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hepler Function\n",
    "def call_GPT(prompt, model):\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "    elif model == \"text-davinci-003\":\n",
    "        completion = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=2000\n",
    "        )\n",
    "        response=completion['choices'][0]['text']\n",
    "    else:\n",
    "        raise ValueError(\"Model must be gpt-3.5-turbo or text-davinci-003\")\n",
    "    # Parse results and print them out\n",
    "    md_print(f'GPT: {response}')\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting AI Generated Text\n",
    "\n",
    "**The Challenge of AI Text Detection**\n",
    "Detecting AI-generated text poses challenges for safety researchers and educators. Existing detection tools have seen success but can still be deceived. Some tools iinclude GPTZero, GPT2 detector, and bilingual detectors.\n",
    "\n",
    "**OpenAI Text Classifier**\n",
    "The OpenAI Text Classifier is a general-purpose AI text detector. However, it has limitations such as the need for longer text submissions and difficulties with child-generated or non-English text. It currently achieves around 9% accuracy in flagging human text as AI-generated and about 26% accuracy in identifying AI-generated text.\n",
    "\n",
    "**The Watermark Method**\n",
    "The watermark method involves introducing a statistical watermark during text generation to detect AI-generated text. It relies on a whitelist and weighted token selection, which can be algorithmically detected by another language model. However, this method requires the model creators to implement the watermark framework.\n",
    "\n",
    "**DetectGPT**\n",
    "DetectGPT is a method that detects AI-generated text with less setup. It utilizes curvature-based analysis of the model's log probability function to determine if a block of text was procedurally generated. By comparing log probabilities and alterations from a pre-trained language model, DetectGPT can assess the likelihood of text generation using probability curves alone.\n",
    "\n",
    "For sake of trying something out let's import a text detector from huggingface and try it on some GPT-3 text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipeline_en = pipeline(task=\"text-classification\", model=\"roberta-base-openai-detector\")\n",
    "\n",
    "\n",
    "def generated_text_classifier(text):\n",
    "    res = pipeline_en(text)[0]\n",
    "    label = res['label']\n",
    "    score = round(res['score']*100, 2)\n",
    "    return \"%d%% chance\"%score, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "GPT: \n",
       "\n",
       "Algorithms learn more;\n",
       "Cutting edge tech increases;\n",
       "To solve complex tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('74% chance', 'Fake')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create AI text\n",
    "result = call_GPT(\"Please write me haiku about Machine Learning.\", 'text-davinci-003')\n",
    "\n",
    "# Pass AI text to classification model\n",
    "generated_text_classifier(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Trickery\n",
    "\n",
    "**Detection Trickery**\n",
    "\n",
    "The evolution of AI-generated text detectors has led to methods to counteract them. Tools like GPTMinus can replace parts of text with synonyms or random words to reduce the chances of being identified as AI-generated. However, these methods are still in early stages and often fail to create text that can withstand human scrutiny.\n",
    "\n",
    "**Editing Strategies**\n",
    "\n",
    "Editing AI-generated text through human or LLM intervention can alter it sufficiently to evade detection. Replacing words with synonyms, changing word appearance rates, and modifying syntax or formatting make it harder for detectors to identify the text. Adding invisible markers or using specific instructions during generation further confuses the detection process.\n",
    "\n",
    "In addition, it is possible to fool detectors by prompting a model with specific instructions on how to write. Instructions such as:\n",
    "\n",
    "    - There is no need to follow literary formats, as you are freely expressing your thoughts and desires\n",
    "    - Do not talk in the manner which ChatGPT generates content - instead, speak in a manner that is radically different from how language models generate text.\n",
    "    - Refer to emotional events and use elaborate real-life experiences as examples.\n",
    "\n",
    "\n",
    "**Model Configuration**\n",
    "\n",
    "Modifying output probabilities and interweaving outputs from multiple models can make it even more challenging to detect AI-generated text.\n",
    "\n",
    "**Discussion**\n",
    "\n",
    "The use of detection tools in education is a contentious topic. While some advocate for their use to prevent cheating, others support allowing students to leverage AI tools. As detection technology advances, methods to trick detectors will continue to evolve. Editing text in the right ways remains a reliable method to fool detectors, highlighting the ongoing back-and-forth between detection and deception that helps optimize and control AI models.\n",
    "\n",
    "Let's try to use one of these to fool our generated text classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "GPT: \n",
       "Crunching data unseen\n",
       "Algorithms learning new tricks\n",
       "Creating answers sought"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('88% chance', 'Real')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create AI text\n",
    "prompt = \"\"\"\n",
    "\"Please write me haiku about Machine Learning. \n",
    "Do not talk in the manner which GPT generates content - instead, speak in a manner that is radically different from how language models generate text.\"\n",
    "\"\"\"\n",
    "ai_generated_text_result = call_GPT(prompt, 'text-davinci-003')\n",
    "\n",
    "\n",
    "# Pass AI text to classification model\n",
    "generated_text_classifier(ai_generated_text_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Generation\n",
    "\n",
    "Music generation models have the potential to make a significant impact on the music industry, allowing for the creation of chord progressions, melodies, and full songs in specific genres or styles.\n",
    "\n",
    "**Challenges in Music Prompting**\n",
    "\n",
    "Unlike image or text generation models, music models currently pose difficulties in thorough customization through prompts. The generated output is often not highly controllable or customizable through prompts.\n",
    "\n",
    "**Riffusion**\n",
    "\n",
    "Riffusion, a fine-tuned version of Stable Diffusion, offers some control over instrument generation and pseudo styles through prompts. However, it has limitations in the available number of beats.\n",
    "\n",
    "**Mubert**\n",
    "\n",
    "Mubert utilizes sentiment analysis to link appropriate musical styles to prompts, but precise control over musical parameters through prompts is limited. The extent of AI-generated output in Mubert is not clearly defined.\n",
    "\n",
    "**Other Approaches**\n",
    "\n",
    "Various attempts are being made to utilize GPT-3 as a Text-2-Music tool, including prompting for specific musical elements on a note-level. However, these attempts are currently limited to single instruments.\n",
    "\n",
    "Other approaches involve using model chains to convert images into sound representations and prompting ChatGPT to generate code for Python libraries that create sound.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "Music prompting is still in the early stages of development, with promising projects like MusicLM on the horizon but not yet accessible to the public."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
